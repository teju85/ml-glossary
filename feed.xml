<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="https://teju85.github.io/ml-glossary/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teju85.github.io/ml-glossary/" rel="alternate" type="text/html" /><updated>2019-02-06T14:19:53+05:30</updated><id>https://teju85.github.io/ml-glossary/</id><title type="html">ML Glossary</title><subtitle>A glossary of ML keywords.</subtitle><entry><title type="html">weights</title><link href="https://teju85.github.io/ml-glossary/terms/weights.html" rel="alternate" type="text/html" title="weights" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/weights</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/weights.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">variance</title><link href="https://teju85.github.io/ml-glossary/terms/variance.html" rel="alternate" type="text/html" title="variance" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/variance</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/variance.html">&lt;p&gt;Variance is the amount of change in the target estimation with a different
training data.&lt;/p&gt;

&lt;p&gt;Ideally, this estimate should not vary a lot. But some ML algorithms have high
variance than others. In such cases, they are sensitive to the specifics of the
training data.&lt;/p&gt;</content><author><name></name></author><summary type="html">Variance is the amount of change in the target estimation with a different training data.</summary></entry><entry><title type="html">unitary matrix</title><link href="https://teju85.github.io/ml-glossary/terms/unitary-matrix.html" rel="alternate" type="text/html" title="unitary matrix" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/unitary-matrix</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/unitary-matrix.html">&lt;p&gt;A square matrix &lt;em&gt;M&lt;/em&gt; is called unitary if:
&lt;script type=&quot;math/tex&quot;&gt;M . M* = M* . M = I&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;M = a real/complex square matrix&lt;/li&gt;
  &lt;li&gt;M* = conjugate transpose of M&lt;/li&gt;
  &lt;li&gt;I = identity matrix&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">A square matrix M is called unitary if: M = a real/complex square matrix M* = conjugate transpose of M I = identity matrix</summary></entry><entry><title type="html">underfitting</title><link href="https://teju85.github.io/ml-glossary/terms/underfitting.html" rel="alternate" type="text/html" title="underfitting" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/underfitting</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/underfitting.html">&lt;p&gt;Underfitting is a scenario in ML where the model is unable to capture the
general structure of the data and its patterns.&lt;/p&gt;

&lt;p&gt;This typically occurs if the model has less parameters than is required by the
dataset. In such cases, the model will have poor predictive ability.&lt;/p&gt;</content><author><name></name></author><summary type="html">Underfitting is a scenario in ML where the model is unable to capture the general structure of the data and its patterns.</summary></entry><entry><title type="html">svd</title><link href="https://teju85.github.io/ml-glossary/terms/svd.html" rel="alternate" type="text/html" title="svd" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/svd</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/svd.html">&lt;p&gt;SVD is a matrix factorization technique.&lt;/p&gt;

&lt;p&gt;For a given matrix, there exists a decomposition of it which can be written as
follows:
&lt;script type=&quot;math/tex&quot;&gt;M = U . \Sigma . V^*&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;M = the input matrix of dimension m x n.&lt;/li&gt;
  &lt;li&gt;U = m x m unitary matrix&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; = m x n diagonal matrix&lt;/li&gt;
  &lt;li&gt;V = n x n unitary matrix&lt;/li&gt;
  &lt;li&gt;V* = conjugate transpose of V&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basic idea behind SVD is that the transformation operation performed by a matrix
M can be viewed as a sequence of the following transformations:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;V* represents the rotation operation applied onto to a unit hypersphere.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; then represents the scaling across the co-ordinate axes.&lt;/li&gt;
  &lt;li&gt;U represents another rotation after scaling operation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SVD is most often used in finding eigen values and vectors of the matrix. For
example: columns of V are eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;M^*.M&lt;/script&gt;. columns of U are eigenvectors
of &lt;script type=&quot;math/tex&quot;&gt;M.M^*&lt;/script&gt; and non-zero elements of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; are sqrt of their eigen values.&lt;/p&gt;</content><author><name></name></author><summary type="html">SVD is a matrix factorization technique.</summary></entry><entry><title type="html">Stochastic Gradient Descent</title><link href="https://teju85.github.io/ml-glossary/terms/stochastic-gradient-descent.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/stochastic-gradient-descent</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/stochastic-gradient-descent.html">&lt;p&gt;This performs gradient descent on each of the samples in the dataset.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta . \nabla_{\theta} J(\theta, x^i, y^i)&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x^i, y^i&lt;/script&gt; iâ€™th input sample and label in the dataset&lt;/li&gt;
  &lt;li&gt;all others as explained in
&lt;a href=&quot;/ml-glossary/terms/gradient-descent.html&quot;&gt;gradient descent&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Runs fast&lt;/li&gt;
  &lt;li&gt;Similar convergence guarantees as in
&lt;a href=&quot;/ml-glossary/terms/batch-gradient-descent.html&quot;&gt;batch gradient descent&lt;/a&gt;,
though one should gradually anneal &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; over time to achieve this.&lt;/li&gt;
  &lt;li&gt;Supports online learning&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Creates high variance updates to the parameters&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">This performs gradient descent on each of the samples in the dataset.</summary></entry><entry><title type="html">SMO</title><link href="https://teju85.github.io/ml-glossary/terms/smo.html" rel="alternate" type="text/html" title="SMO" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/smo</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/smo.html">&lt;p&gt;SMO is an iterative algo for efficiently solving quadratic programming (QP). QP
most commonly arises while solving SVMs.&lt;/p&gt;</content><author><name></name></author><summary type="html">SMO is an iterative algo for efficiently solving quadratic programming (QP). QP most commonly arises while solving SVMs.</summary></entry><entry><title type="html">Simplex Algorithm</title><link href="https://teju85.github.io/ml-glossary/terms/simplex-algorithm.html" rel="alternate" type="text/html" title="Simplex Algorithm" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/simplex-algorithm</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/simplex-algorithm.html">&lt;p&gt;Simplex algorithm is one of the methods to solve linear programming problems.&lt;/p&gt;

&lt;p&gt;It solves the standard form of linear programming problem:
Maximizing &lt;script type=&quot;math/tex&quot;&gt;c^T . x&lt;/script&gt; subject to &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt; all &lt;script type=&quot;math/tex&quot;&gt;x_i &gt;= 0&lt;/script&gt; and all &lt;script type=&quot;math/tex&quot;&gt;b_j &gt;= 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;c = coefficients&lt;/li&gt;
  &lt;li&gt;x = the variables&lt;/li&gt;
  &lt;li&gt;A = a rectangular matrix&lt;/li&gt;
  &lt;li&gt;b = constants&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simplex algorithm iteratively tests adjacent vertices all the while improving
the objective function. This can also be visualized as a traversal along the
edges of a polytope formed by the standard form.&lt;/p&gt;</content><author><name></name></author><summary type="html">Simplex algorithm is one of the methods to solve linear programming problems.</summary></entry><entry><title type="html">semantic segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html" rel="alternate" type="text/html" title="semantic segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/semantic-segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html">&lt;p&gt;Semantic segmentation is a segmentation problem where one has to identify
all category of objects.&lt;/p&gt;

&lt;p&gt;They need not necessarily classify all instances of the same category, though.
Refer to &lt;a href=&quot;/ml-glossary//terms/instance-segmentation.html&quot;&gt;instance segmentation&lt;/a&gt;
for more details on this. Example: chairs, table, people, etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">Semantic segmentation is a segmentation problem where one has to identify all category of objects.</summary></entry><entry><title type="html">segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/segmentation.html" rel="alternate" type="text/html" title="segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/segmentation.html">&lt;p&gt;Image segmentation is just the classification of pixels in an image based on
which objects they are part of.&lt;/p&gt;</content><author><name></name></author><summary type="html">Image segmentation is just the classification of pixels in an image based on which objects they are part of.</summary></entry></feed>