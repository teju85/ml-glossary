<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="https://teju85.github.io/ml-glossary/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teju85.github.io/ml-glossary/" rel="alternate" type="text/html" /><updated>2018-01-28T12:17:03+05:30</updated><id>https://teju85.github.io/ml-glossary/</id><title type="html">ML Glossary</title><subtitle>A glossary of ML keywords.</subtitle><entry><title type="html">weights</title><link href="https://teju85.github.io/ml-glossary/terms/weights.html" rel="alternate" type="text/html" title="weights" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/weights</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/weights.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">variance</title><link href="https://teju85.github.io/ml-glossary/terms/variance.html" rel="alternate" type="text/html" title="variance" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/variance</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/variance.html">&lt;p&gt;Variance is the amount of change in the target estimation with a different
training data.&lt;/p&gt;

&lt;p&gt;Ideally, this estimate should not vary a lot. But some ML algorithms have high
variance than others. In such cases, they are sensitive to the specifics of the
training data.&lt;/p&gt;</content><author><name></name></author><summary type="html">Variance is the amount of change in the target estimation with a different training data.</summary></entry><entry><title type="html">unitary matrix</title><link href="https://teju85.github.io/ml-glossary/terms/unitary-matrix.html" rel="alternate" type="text/html" title="unitary matrix" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/unitary-matrix</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/unitary-matrix.html">&lt;p&gt;A square matrix &lt;em&gt;M&lt;/em&gt; is called unitary if:
&lt;script type=&quot;math/tex&quot;&gt;M . M* = M* . M = I&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;M = a real/complex square matrix&lt;/li&gt;
  &lt;li&gt;M* = conjugate transpose of M&lt;/li&gt;
  &lt;li&gt;I = identity matrix&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">A square matrix M is called unitary if: M = a real/complex square matrix M* = conjugate transpose of M I = identity matrix</summary></entry><entry><title type="html">underfitting</title><link href="https://teju85.github.io/ml-glossary/terms/underfitting.html" rel="alternate" type="text/html" title="underfitting" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/underfitting</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/underfitting.html">&lt;p&gt;Underfitting is a scenario in ML where the model is unable to capture the
general structure of the data and its patterns.&lt;/p&gt;

&lt;p&gt;This typically occurs if the model has less parameters than is required by the
dataset. In such cases, the model will have poor predictive ability.&lt;/p&gt;</content><author><name></name></author><summary type="html">Underfitting is a scenario in ML where the model is unable to capture the general structure of the data and its patterns.</summary></entry><entry><title type="html">svd</title><link href="https://teju85.github.io/ml-glossary/terms/svd.html" rel="alternate" type="text/html" title="svd" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/svd</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/svd.html">&lt;p&gt;SVD is a matrix factorization technique.&lt;/p&gt;

&lt;p&gt;For a given matrix, there exists a decomposition of it which can be written as
follows:
&lt;script type=&quot;math/tex&quot;&gt;M = U . \Sigma . V^*&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;M = the input matrix of dimension m x n.&lt;/li&gt;
  &lt;li&gt;U = m x m unitary matrix&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; = m x n diagonal matrix&lt;/li&gt;
  &lt;li&gt;V = n x n unitary matrix&lt;/li&gt;
  &lt;li&gt;V* = conjugate transpose of V&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SVD is most often used in finding eigen values and vectors of the matrix.&lt;/p&gt;</content><author><name></name></author><summary type="html">SVD is a matrix factorization technique.</summary></entry><entry><title type="html">stochastic gradient descent</title><link href="https://teju85.github.io/ml-glossary/terms/sgd.html" rel="alternate" type="text/html" title="stochastic gradient descent" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/sgd</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/sgd.html">&lt;p&gt;Iterative method to minimize the cost-function associated with ML models.&lt;/p&gt;

&lt;p&gt;This is one of the most commonly used minimization procedures in ML.&lt;/p&gt;

&lt;p&gt;Gradient Descent, in general, are iterative and stochastic approximation to
minimize the function of interest. This function is called as an objective
function or cost function, in the case of Machine Learning. The process of
minimizing this function leads to a “learned” model.&lt;/p&gt;</content><author><name></name></author><summary type="html">Iterative method to minimize the cost-function associated with ML models.</summary></entry><entry><title type="html">semantic segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html" rel="alternate" type="text/html" title="semantic segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/semantic-segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html">&lt;p&gt;Semantic segmentation is a segmentation problem where one has to identify
all category of objects.&lt;/p&gt;

&lt;p&gt;They need not necessarily classify all instances of the same category, though.
Refer to &lt;a href=&quot;/ml-glossary//terms/instance-segmentation.html&quot;&gt;instance segmentation&lt;/a&gt;
for more details on this. Example: chairs, table, people, etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">Semantic segmentation is a segmentation problem where one has to identify all category of objects.</summary></entry><entry><title type="html">segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/segmentation.html" rel="alternate" type="text/html" title="segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/segmentation.html">&lt;p&gt;Image segmentation is just the classification of pixels in an image based on
which objects they are part of.&lt;/p&gt;</content><author><name></name></author><summary type="html">Image segmentation is just the classification of pixels in an image based on which objects they are part of.</summary></entry><entry><title type="html">regression tree</title><link href="https://teju85.github.io/ml-glossary/terms/regression-tree.html" rel="alternate" type="text/html" title="regression tree" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/regression-tree</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/regression-tree.html">&lt;p&gt;A special case of decision trees where the outcome is a real number.
Eg: stock price.&lt;/p&gt;</content><author><name></name></author><summary type="html">A special case of decision trees where the outcome is a real number. Eg: stock price.</summary></entry><entry><title type="html">random forests</title><link href="https://teju85.github.io/ml-glossary/terms/random-forests.html" rel="alternate" type="text/html" title="random forests" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/random-forests</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/random-forests.html">&lt;p&gt;Random forests are a type of bagging.&lt;/p&gt;

&lt;p&gt;These help avoid the overfitting problem seen with decision trees.&lt;/p&gt;</content><author><name></name></author><summary type="html">Random forests are a type of bagging.</summary></entry></feed>