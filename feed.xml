<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="https://teju85.github.io/ml-glossary/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teju85.github.io/ml-glossary/" rel="alternate" type="text/html" /><updated>2018-01-24T10:24:55+05:30</updated><id>https://teju85.github.io/ml-glossary/</id><title type="html">ML Glossary</title><subtitle>A glossary of ML keywords.</subtitle><entry><title type="html">weights</title><link href="https://teju85.github.io/ml-glossary/terms/weights.html" rel="alternate" type="text/html" title="weights" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/weights</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/weights.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">variance</title><link href="https://teju85.github.io/ml-glossary/terms/variance.html" rel="alternate" type="text/html" title="variance" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/variance</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/variance.html">&lt;p&gt;Variance is the amount of change in the target estimation with a different
training data.&lt;/p&gt;

&lt;p&gt;Ideally, this estimate should not vary a lot. But some ML algorithms have high
variance than others. In such cases, they are sensitive to the specifics of the
training data.&lt;/p&gt;</content><author><name></name></author><summary type="html">Variance is the amount of change in the target estimation with a different training data.</summary></entry><entry><title type="html">underfitting</title><link href="https://teju85.github.io/ml-glossary/terms/underfitting.html" rel="alternate" type="text/html" title="underfitting" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/underfitting</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/underfitting.html">&lt;p&gt;Underfitting is a scenario in ML where the model is unable to capture the
general structure of the data and its patterns.&lt;/p&gt;

&lt;p&gt;This typically occurs if the model has less parameters than is required by the
dataset. In such cases, the model will have poor predictive ability.&lt;/p&gt;</content><author><name></name></author><summary type="html">Underfitting is a scenario in ML where the model is unable to capture the general structure of the data and its patterns.</summary></entry><entry><title type="html">stochastic gradient descent</title><link href="https://teju85.github.io/ml-glossary/terms/sgd.html" rel="alternate" type="text/html" title="stochastic gradient descent" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/sgd</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/sgd.html">&lt;p&gt;Iterative method to minimize the cost-function associated with ML models.&lt;/p&gt;

&lt;p&gt;This is one of the most commonly used minimization procedures in ML.&lt;/p&gt;

&lt;p&gt;Gradient Descent, in general, are iterative and stochastic approximation to
minimize the function of interest. This function is called as an objective
function or cost function, in the case of Machine Learning. The process of
minimizing this function leads to a “learned” model.&lt;/p&gt;</content><author><name></name></author><summary type="html">Iterative method to minimize the cost-function associated with ML models.</summary></entry><entry><title type="html">semantic segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html" rel="alternate" type="text/html" title="semantic segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/semantic-segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/semantic-segmentation.html">&lt;p&gt;Semantic segmentation is a segmentation problem where one has to identify
all category of objects.&lt;/p&gt;

&lt;p&gt;They need not necessarily classify all instances of the same category, though.
Refer to &lt;a href=&quot;/ml-glossary//terms/instance-segmentation.html&quot;&gt;instance segmentation&lt;/a&gt;
for more details on this. Example: chairs, table, people, etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">Semantic segmentation is a segmentation problem where one has to identify all category of objects.</summary></entry><entry><title type="html">segmentation</title><link href="https://teju85.github.io/ml-glossary/terms/segmentation.html" rel="alternate" type="text/html" title="segmentation" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/segmentation</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/segmentation.html">&lt;p&gt;Image segmentation is just the classification of pixels in an image based on
which objects they are part of.&lt;/p&gt;</content><author><name></name></author><summary type="html">Image segmentation is just the classification of pixels in an image based on which objects they are part of.</summary></entry><entry><title type="html">regression tree</title><link href="https://teju85.github.io/ml-glossary/terms/regression-tree.html" rel="alternate" type="text/html" title="regression tree" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/regression-tree</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/regression-tree.html">&lt;p&gt;A special case of decision trees where the outcome is a real number.
Eg: stock price.&lt;/p&gt;</content><author><name></name></author><summary type="html">A special case of decision trees where the outcome is a real number. Eg: stock price.</summary></entry><entry><title type="html">random forests</title><link href="https://teju85.github.io/ml-glossary/terms/random-forests.html" rel="alternate" type="text/html" title="random forests" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/random-forests</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/random-forests.html">&lt;p&gt;Random forests are a type of bagging.&lt;/p&gt;

&lt;p&gt;These help avoid the overfitting problem seen with decision trees.&lt;/p&gt;</content><author><name></name></author><summary type="html">Random forests are a type of bagging.</summary></entry><entry><title type="html">parameters</title><link href="https://teju85.github.io/ml-glossary/terms/parameters.html" rel="alternate" type="text/html" title="parameters" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/parameters</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/parameters.html">&lt;p&gt;Parameters are the ones that are learnt from the data during training process.&lt;/p&gt;

&lt;p&gt;They could be as simple as a scalar value like &lt;em&gt;bias&lt;/em&gt; in the case of a neural
net layer, or a 4-D &lt;em&gt;filter&lt;/em&gt; in the case of a convolutional layer. They are
typically learnt using iterative methods like SGD, Adam, etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">Parameters are the ones that are learnt from the data during training process.</summary></entry><entry><title type="html">overfitting</title><link href="https://teju85.github.io/ml-glossary/terms/overfitting.html" rel="alternate" type="text/html" title="overfitting" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/overfitting</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/overfitting.html">&lt;p&gt;Overfitting is a scenario in ML where the model has sort of memorized the data
and its patterns.&lt;/p&gt;

&lt;p&gt;This typically occurs if the model has more parameters than can be justified for
the dataset. In such cases, the model becomes too sensitive to the data and thus
may not generalize well to future observation points.&lt;/p&gt;</content><author><name></name></author><summary type="html">Overfitting is a scenario in ML where the model has sort of memorized the data and its patterns.</summary></entry></feed>