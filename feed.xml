<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="http://localhost:4000/ml-glossary/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/ml-glossary/" rel="alternate" type="text/html" /><updated>2018-01-22T09:51:37+05:30</updated><id>http://localhost:4000/ml-glossary/</id><title type="html">ML Glossary</title><subtitle>A glossary of ML keywords.</subtitle><entry><title type="html">stochastic gradient descent</title><link href="http://localhost:4000/ml-glossary/terms/sgd.html" rel="alternate" type="text/html" title="stochastic gradient descent" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>http://localhost:4000/ml-glossary/terms/sgd</id><content type="html" xml:base="http://localhost:4000/ml-glossary/terms/sgd.html">&lt;p&gt;Iterative method to minimize the cost-function associated with ML models.&lt;/p&gt;

&lt;p&gt;This is one of the most commonly used minimization procedures in ML.&lt;/p&gt;

&lt;p&gt;Gradient Descent, in general, are iterative and stochastic approximation to
minimize the function of interest. This function is called as an objective
function or cost function, in the case of Machine Learning. The process of
minimizing this function leads to a “learned” model.&lt;/p&gt;</content><author><name></name></author><summary type="html">Iterative method to minimize the cost-function associated with ML models.</summary></entry><entry><title type="html">parameters</title><link href="http://localhost:4000/ml-glossary/terms/parameters.html" rel="alternate" type="text/html" title="parameters" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>http://localhost:4000/ml-glossary/terms/parameters</id><content type="html" xml:base="http://localhost:4000/ml-glossary/terms/parameters.html">&lt;p&gt;Parameters are the ones that are learnt from the data during training process.&lt;/p&gt;

&lt;p&gt;They could be as simple as a scalar value like &lt;em&gt;bias&lt;/em&gt; in the case of a neural
net layer, or a 4-D &lt;em&gt;filter&lt;/em&gt; in the case of a convolutional layer. They are
typically learnt using iterative methods like SGD, Adam, etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">Parameters are the ones that are learnt from the data during training process.</summary></entry><entry><title type="html">learning rate</title><link href="http://localhost:4000/ml-glossary/terms/learning-rate.html" rel="alternate" type="text/html" title="learning rate" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>http://localhost:4000/ml-glossary/terms/learning-rate</id><content type="html" xml:base="http://localhost:4000/ml-glossary/terms/learning-rate.html">&lt;p&gt;It is a hyper-parameter that controls the step-size during gradient-based
updates for the parameters of the model.&lt;/p&gt;

&lt;p&gt;In other words, this is the &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; that comes up in the gradient descent
update equations as in:
&lt;script type=&quot;math/tex&quot;&gt;x_{n+1} = x_{n} - \eta F'(x_{n})&lt;/script&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">It is a hyper-parameter that controls the step-size during gradient-based updates for the parameters of the model.</summary></entry><entry><title type="html">hyper-parameters</title><link href="http://localhost:4000/ml-glossary/terms/hyper-parameters.html" rel="alternate" type="text/html" title="hyper-parameters" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>http://localhost:4000/ml-glossary/terms/hyper-parameters</id><content type="html" xml:base="http://localhost:4000/ml-glossary/terms/hyper-parameters.html">&lt;p&gt;Hyper-parameters are the parameters that define the model itself and how it
is learnt from data.&lt;/p&gt;

&lt;p&gt;These are the parameters that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;cannot be learnt during the training process&lt;/li&gt;
  &lt;li&gt;define the model complexity&lt;/li&gt;
  &lt;li&gt;define the learning process
For example: learning algorithm, learning-rate, number of layers in a neural
net, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, one needs to perform a search in the hyper-parameter space in order to
discover the best performing model.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hyper-parameters are the parameters that define the model itself and how it is learnt from data.</summary></entry><entry><title type="html">hyper-parameter optimization</title><link href="http://localhost:4000/ml-glossary/terms/hyper-parameter-optimization.html" rel="alternate" type="text/html" title="hyper-parameter optimization" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>http://localhost:4000/ml-glossary/terms/hyper-parameter-optimization</id><content type="html" xml:base="http://localhost:4000/ml-glossary/terms/hyper-parameter-optimization.html">&lt;p&gt;The process of figuring out the optimal set of hyper-parameters for the given
dataset, model and learning algorithm.&lt;/p&gt;

&lt;p&gt;One naive way of doing this is to perform a grid-search over all possible
combination of hyper-parameter values.&lt;/p&gt;</content><author><name></name></author><summary type="html">The process of figuring out the optimal set of hyper-parameters for the given dataset, model and learning algorithm.</summary></entry></feed>