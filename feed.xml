<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://teju85.github.io/ml-glossary/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teju85.github.io/ml-glossary/" rel="alternate" type="text/html" /><updated>2020-07-29T08:47:41+05:30</updated><id>https://teju85.github.io/ml-glossary/feed.xml</id><title type="html">ML Glossary</title><subtitle>A glossary of ML keywords.</subtitle><entry><title type="html">weights</title><link href="https://teju85.github.io/ml-glossary/terms/weights.html" rel="alternate" type="text/html" title="weights" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/weights</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/weights.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Adagrad</title><link href="https://teju85.github.io/ml-glossary/terms/adagrad.html" rel="alternate" type="text/html" title="Adagrad" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/adagrad</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/adagrad.html">&lt;p&gt;This method updates the learning rate to each parameter. Large updates for
infrequent params, small updates for the frequent ones.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_t = \nabla_{\theta_{t-1}} J(\theta_{t-1})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_t = \sqrt{\epsilon + \Sigma_k (g_k)^2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\forall k = [0, t]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_t = \theta_{t-1} - \frac{\eta}{f_t} g_t&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; = smoothing parameter&lt;/li&gt;
  &lt;li&gt;all others are as explained in
&lt;a href=&quot;/ml-glossary/terms/nesterov-accelerated-gradient.html&quot;&gt;NAG&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Works well for sparse data&lt;/li&gt;
  &lt;li&gt;improves robustness of SGD&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Due to accumulated squared gradients, it can shrink the learning rate very badly&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">This method updates the learning rate to each parameter. Large updates for infrequent params, small updates for the frequent ones.</summary></entry><entry><title type="html">Adam</title><link href="https://teju85.github.io/ml-glossary/terms/adam.html" rel="alternate" type="text/html" title="Adam" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/adam</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/adam.html">&lt;p&gt;Same as &lt;a href=&quot;/ml-glossary/terms/adadelta.html&quot;&gt;AdaDelta&lt;/a&gt; and
&lt;a href=&quot;/ml-glossary/terms/rmsprop.html&quot;&gt;RMSProp&lt;/a&gt;, stores the
running average of past gradients to use them as moment.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_t = \nabla_{\theta_{t-1}} J(\theta_{t-1})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m'_t = \frac{m_t}{1 - \beta_1^t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v'_t = \frac{v_t}{1 - \beta_2^t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta \theta_t = - \frac{\eta}{\sqrt{v'_t} + \epsilon} m'_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_t = \theta_{t-1} + \Delta \theta_t&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;m_t&lt;/script&gt; = first moment of gradients&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;v_t&lt;/script&gt; = second moment of gradients&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta_1, \beta_2&lt;/script&gt; = decay rates&lt;/li&gt;
  &lt;li&gt;all others are as explained in
&lt;a href=&quot;/ml-glossary/terms/adagrad.html&quot;&gt;AdaGrad&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Same as AdaDelta and RMSProp, stores the running average of past gradients to use them as moment.</summary></entry><entry><title type="html">Adamax</title><link href="https://teju85.github.io/ml-glossary/terms/adamax.html" rel="alternate" type="text/html" title="Adamax" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/adamax</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/adamax.html">&lt;p&gt;Same as &lt;a href=&quot;/ml-glossary/terms/adam.html&quot;&gt;Adam&lt;/a&gt;, but uses the
&lt;script type=&quot;math/tex&quot;&gt;l_{\infty}&lt;/script&gt; norm in the running average of past gradients.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = \beta_2 v_{t-1} + (1 - \beta_2) |g_t|^\infty&lt;/script&gt;

&lt;p&gt;OR&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = max(\beta_2 v_{t-1}, |g_t|)&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;terms and remaining equations are as in &lt;a href=&quot;/ml-glossary/terms/adam.html&quot;&gt;Adam&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Same as Adam, but uses the norm in the running average of past gradients.</summary></entry><entry><title type="html">AMSgrad</title><link href="https://teju85.github.io/ml-glossary/terms/amsgrad.html" rel="alternate" type="text/html" title="AMSgrad" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/amsgrad</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/amsgrad.html">&lt;p&gt;Similar to &lt;a href=&quot;/ml-glossary/terms/adam.html&quot;&gt;Adam&lt;/a&gt; but keeps the
previous max gradient to avoid converging to local optima.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v'_t = max(v'_{t-1}, v_t)&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;all others and remaining equations are as explained in
&lt;a href=&quot;/ml-glossary/terms/adam.html&quot;&gt;Adam&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Similar to Adam but keeps the previous max gradient to avoid converging to local optima.</summary></entry><entry><title type="html">bagging</title><link href="https://teju85.github.io/ml-glossary/terms/bagging.html" rel="alternate" type="text/html" title="bagging" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/bagging</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/bagging.html">&lt;p&gt;An ensemble ML technique to reduce overfitting and variance.&lt;/p&gt;

&lt;p&gt;From a training dataset T, generate ‘n’ new datasets by sampling observations in
T with replacement. Then train ‘n’ models separately on these sampled datsets.
Finally, use a consensus among these models to get final prediction.&lt;/p&gt;

&lt;p&gt;This technique is typically used with decision trees, as they are more
susceptible to overfitting.&lt;/p&gt;</content><author><name></name></author><summary type="html">An ensemble ML technique to reduce overfitting and variance.</summary></entry><entry><title type="html">Batch Gradient Descent</title><link href="https://teju85.github.io/ml-glossary/terms/batch-gradient-descent.html" rel="alternate" type="text/html" title="Batch Gradient Descent" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/batch-gradient-descent</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/batch-gradient-descent.html">&lt;p&gt;This performs gradient descent with the entire dataset.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta . \nabla_{\theta} J(\theta)&lt;/script&gt;

&lt;p&gt;Where the meaning of each is as defined in
&lt;a href=&quot;/ml-glossary/terms/gradient-descent.html&quot;&gt;gradient descent&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Guaranteed to converge to global minima for convex &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Converges to local minima for other surface types.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Doesn’t work for datasets needing to be out-of-core.&lt;/li&gt;
  &lt;li&gt;Thus, no online learning&lt;/li&gt;
  &lt;li&gt;Runs slow&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">This performs gradient descent with the entire dataset.</summary></entry><entry><title type="html">bias</title><link href="https://teju85.github.io/ml-glossary/terms/bias.html" rel="alternate" type="text/html" title="bias" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/bias</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/bias.html">&lt;p&gt;Bias is the amount of assumptions made by the ML algo in order to learn a target
function from training data.&lt;/p&gt;

&lt;p&gt;Ideally, these assumptions should be as little as possible. That way, the ML
algo can be as flexible as posisble. But some ML algorithms have high bias than
others. In such cases, their predictive ability gets hampered.&lt;/p&gt;</content><author><name></name></author><summary type="html">Bias is the amount of assumptions made by the ML algo in order to learn a target function from training data.</summary></entry><entry><title type="html">CART</title><link href="https://teju85.github.io/ml-glossary/terms/cart.html" rel="alternate" type="text/html" title="CART" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/cart</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/cart.html">&lt;p&gt;Short for Classification And Regression Tree, an umbrella term for both
classification and regression decision trees.&lt;/p&gt;</content><author><name></name></author><summary type="html">Short for Classification And Regression Tree, an umbrella term for both classification and regression decision trees.</summary></entry><entry><title type="html">Cholesky Decomp</title><link href="https://teju85.github.io/ml-glossary/terms/cholesky-decomp.html" rel="alternate" type="text/html" title="Cholesky Decomp" /><published>2018-01-01T00:00:00+05:30</published><updated>2018-01-01T00:00:00+05:30</updated><id>https://teju85.github.io/ml-glossary/terms/cholesky-decomp</id><content type="html" xml:base="https://teju85.github.io/ml-glossary/terms/cholesky-decomp.html">&lt;p&gt;Cholesky decomposition is a hermitian positive-definite matrix factorization
technique.&lt;/p&gt;

&lt;p&gt;For a given hermitian positive-definite matrix A, there exists a decomposition
of the form:
&lt;script type=&quot;math/tex&quot;&gt;A = L . L^*&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A = the input matrix of dimension n x n.&lt;/li&gt;
  &lt;li&gt;L = lower triangular matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cholesky-decomp is ~2x faster than LU-decomp for solving linear systems, when
applicable.&lt;/p&gt;</content><author><name></name></author><summary type="html">Cholesky decomposition is a hermitian positive-definite matrix factorization technique.</summary></entry></feed>