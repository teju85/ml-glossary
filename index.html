<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="stylesheet" href="/ml-glossary/assets/main.css"> <link rel="stylesheet" href="/ml-glossary/assets/filter.css"> <script rel="src" src="/ml-glossary/assets/filter.js"></script> </head> <body> <header class="site-header" role="banner"> <div class="wrapper"> <a class="site-title" rel="author" href="/ml-glossary/">ML Glossary</a> <nav class="site-nav"> <input type="checkbox" id="nav-trigger" class="nav-trigger" /> <label for="nav-trigger"> <span class="menu-icon"> <svg viewBox="0 0 18 15" width="18px" height="15px"> <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/> <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/> <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/> </svg> </span> </label> <div class="trigger"> </div> </nav> </div> </header> <main class="page-content" aria-label="Content"> <div class="wrapper"> <div class="home"> <p><input type="text" id="termSearch" onkeyup="filterTerms()" placeholder="Filter terms..." /></p> <ul class="post-list" id="termsList"> <li> <h3> <a class="post-link" href="/ml-glossary/terms/adadelta.html"> AdaDelta </a> </h3> <span>Same as <a href="/ml-glossary/terms/adagrad.html">AdaGrad</a>, but, the running average is limited to a fixed window. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/adagrad.html"> Adagrad </a> </h3> <span>This method updates the learning rate to each parameter. Large updates for infrequent params, small updates for the frequent ones. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/adam.html"> Adam </a> </h3> <span>Same as <a href="/ml-glossary/terms/adadelta.html">AdaDelta</a> and <a href="/ml-glossary/terms/rmsprop.html">RMSProp</a>, stores the running average of past gradients to use them as moment. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/adamax.html"> Adamax </a> </h3> <span>Same as <a href="/ml-glossary/terms/adam.html">Adam</a>, but uses the <script type="math/tex">l_{\infty}</script> norm in the running average of past gradients. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/amsgrad.html"> AMSgrad </a> </h3> <span>Similar to <a href="/ml-glossary/terms/adam.html">Adam</a> but keeps the previous max gradient to avoid converging to local optima. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/bagging.html"> bagging </a> </h3> <span>An ensemble ML technique to reduce overfitting and variance. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/batch-gradient-descent.html"> Batch Gradient Descent </a> </h3> <span>This performs gradient descent with the entire dataset. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/bias.html"> bias </a> </h3> <span>Bias is the amount of assumptions made by the ML algo in order to learn a target function from training data. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/cart.html"> CART </a> </h3> <span>Short for Classification And Regression Tree, an umbrella term for both classification and regression decision trees. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/cholesky-decomp.html"> Cholesky Decomp </a> </h3> <span>Cholesky decomposition is a hermitian positive-definite matrix factorization technique. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/classification-tree.html"> classification tree </a> </h3> <span>A special case of decision trees where the outcome is the class of the input data. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/conjugate-transpose.html"> conjugate transpose </a> </h3> <span>Conjugate transpose of a complex matrix <em>M</em> is performed by taking transpose of <em>M</em>, followed by complex conjugate of every elements. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/covariance.html"> covariance </a> </h3> <span>Covariance is a measure of joint-variability of two random variables. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/decision-trees.html"> decision trees </a> </h3> <span>A decision tree is a flowchart which each node performing a condition check on an attribute in the input data. This check is then recursively performed on the non-leaf child nodes, until a leaf node is encountered. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/factor-analysis.html"> factor analysis </a> </h3> <span>Factor Analysis is a way of describing observed (and possibly) correlated input variables in terms of lesser number of unobserved or latent variables. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/gbt.html"> gbt </a> </h3> <span>Popular gradient boosting method where the weak learners are decision trees. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/givens-rotation.html"> Givens Rotation </a> </h3> <span>Product of a givens rotation matrix with a vector represents counter clockwise rotation on a given plane, by given degrees/radians. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/gradient-boosting.html"> gradient boosting </a> </h3> <span>An ML technique to build a model using an ensemble of weak learners </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/gradient-descent.html"> Gradient Descent </a> </h3> <span>Gradient Descent, in general, are iterative and stochastic approximation to minimize the function of interest. This function is called as an objective function or cost function, in the case of Machine Learning. The process of minimizing this function leads to a “learned” model. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/hermitian-matrix.html"> Hermitian matrix </a> </h3> <span>A matrix <script type="math/tex">M</script> is said to be hermitian if it is equal to its conjugate transpose. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/hyper-parameter-optimization.html"> hyper-parameter optimization </a> </h3> <span>The process of figuring out the optimal set of hyper-parameters for the given dataset, model and learning algorithm. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/hyper-parameters.html"> hyper-parameters </a> </h3> <span>Hyper-parameters are the parameters that define the model itself and how it is learnt from data. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/instance-segmentation.html"> instance segmentation </a> </h3> <span>Instance segmentation is a segmentation problem where one has to identify individual objects, even if they are overlapping and are of the same “type” of object. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/lasso.html"> lasso </a> </h3> <span>Lasso is a regression analysis technique to perform both variable selection as well as model regularization. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/latent-variable.html"> latent variable </a> </h3> <span>Latent variables are the ones that are not directly observed, but are inferred using some sort of mathematical models. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/learning-rate.html"> learning rate </a> </h3> <span>It is a hyper-parameter that controls the step-size during gradient-based updates for the parameters of the model. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/lu.html"> LU </a> </h3> <span>LU decomposition is a square matrix factorization technique. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/markov-chain-monte-carlo.html"> Markov Chain Monte Carlo </a> </h3> <span>MCMC methods help us in estimating the posterior distributions via sampling from a complicated probability distribution. As the name suggests, it consists of 2 parts: Monte Carlo and Markov Chain. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/metropolis-hastings.html"> Metropolis-Hastings </a> </h3> <span>MH is one way of doing MCMC. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/minibatch-gradient-descent.html"> Minibatch Gradient Descent </a> </h3> <span>This performs gradient descent on a smaller subset of samples in the dataset. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/momentum-gradient-descent.html"> Momentum Gradient Descent </a> </h3> <span>To minimize the issues of local-optima convergence of Gradient Descent, one can use momentum. In other words, if the gradients keep changing in direction every iteration, this will dampen such oscillations. And if gradients don’t change too much, this will further enhance their update strength. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/nadam.html"> Nadam </a> </h3> <span>Combines <a href="/ml-glossary/terms/nesterov-accelerated-gradient.html">NAG</a> and <a href="/ml-glossary/terms/adam.html">Adam</a>. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/nesterov-accelerated-gradient.html"> Nesterov Accelerated Gradient </a> </h3> <span>To avoid big jumps due to gradient descent with momentum, NAG first makes the big jump and then corrects it accordingly. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/nonparametric-bayesian.html"> Non-parametric Bayesian </a> </h3> <span>A non-parametric bayesian model is a bayesian model with infinite number of parameters. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/overfitting.html"> overfitting </a> </h3> <span>Overfitting is a scenario in ML where the model has sort of memorized the data and its patterns. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/parameters.html"> parameters </a> </h3> <span>Parameters are the ones that are learnt from the data during training process. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/pca.html"> pca </a> </h3> <span>Statistical technique to convert a set of (possibly) correlated inputs into a set of uncorrelated variables. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/positive-definite-matrix.html"> Positive-definite matrix </a> </h3> <span>A hermitian matrix <script type="math/tex">M</script> is said to be positive-definite if <script type="math/tex">z* M z > 0</script> for-all non-zero vectors <script type="math/tex">z</script>. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/posterior-probability.html"> Posterior Probability </a> </h3> <span>Posterior Probability of a random event is the resulting probability after taking into account, the relevant evidence. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/qr.html"> QR </a> </h3> <span>QR decomposition is a square matrix factorization technique. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/quasi-newton-method.html"> Quasi Newton Method </a> </h3> <span>QN are a family of methods for optimization, based on Newton methods. They, however, do not require the computation of the Hessian matrix. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/random-forests.html"> random forests </a> </h3> <span>Random forests are a type of bagging, with every tree in the forest potentially constructed in parallel by sampling the dataset with replacement. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/regression-tree.html"> regression tree </a> </h3> <span>A special case of decision trees where the outcome is a real number. Eg: stock price. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/rmsprop.html"> RMSProp </a> </h3> <span>Same as <a href="/ml-glossary/terms/adagrad.html">AdaGrad</a>, but, the running average is limited to a fixed window. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/row-stochastic-matrix.html"> Row Stochastic Matrix </a> </h3> <span>A probability matrix is called row stochastic, if each of it’s rows sum upto one. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/segmentation.html"> segmentation </a> </h3> <span>Image segmentation is just the classification of pixels in an image based on which objects they are part of. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/semantic-segmentation.html"> semantic segmentation </a> </h3> <span>Semantic segmentation is a segmentation problem where one has to identify all category of objects. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/simplex-algorithm.html"> Simplex Algorithm </a> </h3> <span>Simplex algorithm is one of the methods to solve linear programming problems. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/smo.html"> SMO </a> </h3> <span>SMO is an iterative algo for efficiently solving quadratic programming (QP). QP most commonly arises while solving SVMs. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/stochastic-average-gradient-descent.html"> Stochastic Average Gradient Descent </a> </h3> <span>This is a hybrid between batch gradient descent and stochastic gradient descent. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/stochastic-gradient-descent.html"> Stochastic Gradient Descent </a> </h3> <span>This performs gradient descent on each of the samples in the dataset. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/svd.html"> svd </a> </h3> <span>SVD is a matrix factorization technique. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/underfitting.html"> underfitting </a> </h3> <span>Underfitting is a scenario in ML where the model is unable to capture the general structure of the data and its patterns. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/unitary-matrix.html"> unitary matrix </a> </h3> <span>A square matrix <em>M</em> is called unitary if: <script type="math/tex">M . M* = M* . M = I</script> <ul> <li>M = a real/complex square matrix</li> <li>M* = conjugate transpose of M</li> <li>I = identity matrix</li> </ul> </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/variance.html"> variance </a> </h3> <span>Variance is the amount of change in the target estimation with a different training data. </span> </li> <li> <h3> <a class="post-link" href="/ml-glossary/terms/weights.html"> weights </a> </h3> <span> </span> </li> </ul> </div> </div> </main> <footer class="site-footer h-card"> <data class="u-url" href="/ml-glossary/"></data> <div class="wrapper"> <div class="footer-col-wrapper"> <div class="footer-col footer-col-3"> <p>A glossary of ML keywords.</p> </div> </div> </div> </footer> </body> </html>
