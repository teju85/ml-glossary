---
layout: page
title: AdaDelta
references:
  - https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning
needmath: true
seealso:
  - gradient-descent
  - nesterov-accelerated-gradient
  - adagrad
---
Same as [AdaGrad]({{ site.baseurl }}{% post_url 2018-01-01-adagrad %}), but,
the running average is limited to a fixed window.

$$g_t = \nabla_{\theta_{t-1}} J(\theta_{t-1})$$

$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2$$

$$ RMS[g]_t = \sqrt{\epsilon + E[g^2]_t}$$

$$\Delta \theta_t = - \frac{\eta}{RMS[g]_t}$$

$$E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) (\Delta \theta_t)^2$$

$$ RMS[\Delta \theta]_t = \sqrt{\epsilon + E[\Delta \theta^2]_t}$$

$$ \Delta \theta_t = -\frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_t} g_t$$

$$\theta_t = \theta_{t-1} + \Delta \theta_t $$

Where:
1. $$\gamma$$ = similar to the momentum parameter
2. all others are as explained in
[AdaGrad]({{ site.baseurl }}{% post_url 2018-01-01-adagrad %})
