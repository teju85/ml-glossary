---
layout: page
title: bagging
aka: bootstrap aggregating
seealso: decision-trees, overfitting, variance
references:
  - https://en.wikipedia.org/wiki/Bootstrap_aggregating
  - http://machine-learning.martinsewell.com/ensembles/bagging/
---
An ensemble ML technique to reduce overfitting and variance.

From a training dataset T, generate 'n' new datasets by sampling observations in
T with replacement. Then train 'n' models separately on these sampled datsets.
Finally, use a consensus among these models to get final prediction.

This technique is typically used with decision trees, as they are more
susceptible to overfitting.
