---
layout: page
title: RMSProp
references:
  - https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning
needmath: true
seealso:
  - gradient-descent
  - nesterov-accelerated-gradient
  - adagrad
---
Same as [Adagrad]({{ site.baseurl }}{% post_url 2018-01-01-adagrad %}), but,
the running average is limited to a fixed window. Developed around the same time
as that of [AdaDelta]({{ site.baseurl }}{% post_url 2018-01-01-adadelta %}).

* $$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2$$
* $$ RMS[g]_t = \sqrt{\epsilon + E[g^2]_t}$$
* $$\Delta \theta_t = - \frac{\eta}{RMS[g]_t} g_t$$
* $$\theta_t = \theta_{t-1} + \Delta \theta_t $$

Where:
1. all other equations and parameters are as explained in
   [Adagrad]({{ site.baseurl }}{% post_url 2018-01-01-adagrad %})
