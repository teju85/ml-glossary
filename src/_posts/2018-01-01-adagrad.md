---
layout: page
title: Adagrad
references:
  - https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning
needmath: true
seealso:
  - gradient-descent
  - nesterov-accelerated-gradient
---
This method updates the learning rate to each parameter. Large updates for
infrequent params, small updates for the frequent ones.

$$g_t = \nabla_{\theta_{t-1}} J(\theta_{t-1})$$

$$f_t = \sqrt{\epsilon + \Sigma_k (g_k)^2}$$

$$\forall k = [0, t]$$

$$\theta_t = \theta_{t-1} - \frac{\eta}{f_t} g_t$$

Where:
1. $$\epsilon$$ = smoothing parameter
2. all others are as explained in
[NAG]({{ site.baseurl }}{% post_url 2018-01-01-nesterov-accelerated-gradient %})

Pros:
1. Works well for sparse data
2. improves robustness of SGD

Cons:
1. Due to accumulated squared gradients, it can shrink the learning rate very badly
