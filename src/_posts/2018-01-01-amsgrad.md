---
layout: page
title: AMSgrad
references:
  - http://ruder.io/optimizing-gradient-descent/index.html#amsgrad
needmath: true
seealso:
  - adam
---
Similar to [Adam]({{ site.baseurl }}{% post_url 2018-01-01-adam %}) but keeps the
previous max gradient to avoid converging to local optima.

$$v'_t = max(v'_{t-1}, v_t)$$

Where:
1. all others and remaining equations are as explained in
[Adam]({{ site.baseurl }}{% post_url 2018-01-01-adam %})
