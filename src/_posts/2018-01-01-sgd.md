---
layout: page
title: stochastic gradient descent
aka: SGD, incremental gradient descent
references: https://en.wikipedia.org/wiki/Stochastic_gradient_descent
---
Iterative method to minimize the cost-function associated with ML models.

This is one of the most commonly used minimization procedures in ML.

Gradient Descent, in general, are iterative and stochastic approximation to
minimize the function of interest. This function is called as an objective
function or cost function, in the case of Machine Learning. The process of
minimizing this function leads to a "learned" model.
