---
layout: page
title: Nadam
aka: Nesterov accelerated adaptive moment estimation
references:
  - http://ruder.io/optimizing-gradient-descent/index.html#nadam
needmath: true
seealso:
  - nesterov-accelerated-gradient
  - adam
---
Combines [NAG]({{ site.baseurl }}{% post_url 2018-01-01-nesterov-accelerated-gradient %})
and [Adam]({{ site.baseurl }}{% post_url 2018-01-01-adam %}).

It uses look-ahead momentum vector directly to the current parameter update

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t$$

$$m'_t = \frac{m_t}{1 - \beta_1^t}$$

$$u_t = \beta_1 m'_{t-1} + \frac{(1 - \beta_1)g_t}{1 - \beta_1^t}$$

$$\Delta \theta_t = - \frac{\eta}{\sqrt{v'_t} + \epsilon} u_t$$

$$\theta_t = \theta_{t-1} + \Delta \theta_t $$

Where:
1. all others are as explained in
[Adam]({{ site.baseurl }}{% post_url 2018-01-01-adam %})
